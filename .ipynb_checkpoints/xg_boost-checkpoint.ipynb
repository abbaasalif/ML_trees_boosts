{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FLud1n-3pVm"
   },
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.5.1-py3-none-win_amd64.whl (106.6 MB)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from xgboost) (1.19.2)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from xgboost) (1.5.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sO8VPU6n3vES"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clDSsF7P33NU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zGpwK5XD386E"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zcksk88u4Ae8"
   },
   "outputs": [],
   "source": [
    "#breast cancer data set where 0 is benigin(not cancerous tumor) and 1 is malignant(cancerous tumor)\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNn2RnST6_Q-"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajhBL-er7Gry"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Y89ctGZ7Mcx"
   },
   "source": [
    "## Training XGBoost on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ude1J0E47SKN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:47:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivqmubzW7dFJ"
   },
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUSZ3zm_7gRD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85  2]\n",
      " [ 1 49]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9781021897810219"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnbCjHgQ8XPn"
   },
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYbfiITD8ZAz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:48:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:48:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:48:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:48:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:48:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:48:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:48:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:48:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:48:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:48:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 96.53 %\n",
      "Standard Deviation: 2.63 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search CV to find better parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 96 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:   21.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:50:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done 960 out of 960 | elapsed:   26.1s finished\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1,\n",
       "                                     enable_categorical=False, gamma=0,\n",
       "                                     gpu_id=-1, importance_type=None,\n",
       "                                     interaction_constraints='',\n",
       "                                     learning_rate=0.300000012,\n",
       "                                     max_delta_step=0, max_depth=6,\n",
       "                                     min_child_weight=1, missing=nan,\n",
       "                                     monotone_constraints='()',\n",
       "                                     n_estimators=100, n_jobs=8,\n",
       "                                     num_parallel_tree=1, predictor='auto',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, subsample=1,\n",
       "                                     tree_method='exact', validate_parameters=1,\n",
       "                                     verbosity=None),\n",
       "             n_jobs=10,\n",
       "             param_grid={'learning_rate': [0.1, 0.01, 0.05],\n",
       "                         'max_depth': range(2, 10),\n",
       "                         'n_estimators': range(60, 220, 40)},\n",
       "             scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'max_depth': range (2, 10, 1),\n",
    "    'n_estimators': range(60, 220, 40),\n",
    "    'learning_rate': [0.1, 0.01, 0.05]\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=classifier,\n",
    "    param_grid=parameters,\n",
    "    scoring = 'roc_auc',\n",
    "    n_jobs = -1,\n",
    "    cv = 10,\n",
    "    verbose=True,\n",
    "    refit=True\n",
    ")\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
       "              max_depth=2, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=140, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'std_fit_time'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(grid_search.cv_results_, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.22139878, 0.17319968, 0.19159856, 0.23499925, 0.13129957,\n",
       "        0.17399914, 0.23459985, 0.28449864, 0.14869874, 0.1910991 ,\n",
       "        0.245999  , 0.29819889, 0.15659924, 0.196099  , 0.25529902,\n",
       "        0.30979891, 0.16439986, 0.20199928, 0.26390004, 0.31339917,\n",
       "        0.16259894, 0.20509963, 0.26329877, 0.31369922, 0.16519918,\n",
       "        0.203599  , 0.26149955, 0.31679945, 0.16160018, 0.20329938,\n",
       "        0.26099927, 0.33689952, 0.09889958, 0.15399957, 0.19049885,\n",
       "        0.23689923, 0.13299932, 0.17729924, 0.24209883, 0.29959919,\n",
       "        0.15479925, 0.20499914, 0.27479863, 0.35449891, 0.16169939,\n",
       "        0.22569892, 0.30109947, 0.39119895, 0.16469944, 0.23879883,\n",
       "        0.31669865, 0.41119938, 0.1698998 , 0.24259903, 0.32249899,\n",
       "        0.41689878, 0.17379882, 0.23959899, 0.32599947, 0.4264992 ,\n",
       "        0.16199925, 0.24349937, 0.32559888, 0.4364995 , 0.09619908,\n",
       "        0.15029919, 0.19369843, 0.23829904, 0.12319942, 0.17179871,\n",
       "        0.22639928, 0.28509872, 0.14869964, 0.19979873, 0.25649903,\n",
       "        0.32049882, 0.15789912, 0.21719968, 0.28379853, 0.36049898,\n",
       "        0.16259949, 0.22319882, 0.28869936, 0.35509901, 0.16939948,\n",
       "        0.22559872, 0.29189904, 0.36369932, 0.16709957, 0.22589889,\n",
       "        0.29519892, 0.35999861, 0.16829963, 0.226899  , 0.29489884,\n",
       "        0.32819829]),\n",
       " 'std_fit_time': array([0.03235754, 0.01820862, 0.00733753, 0.00983829, 0.00643497,\n",
       "        0.00688472, 0.01093768, 0.00508429, 0.00871827, 0.00461342,\n",
       "        0.00397505, 0.00596329, 0.01159504, 0.00533741, 0.00810035,\n",
       "        0.0119148 , 0.01553195, 0.00598304, 0.01069077, 0.01049965,\n",
       "        0.009276  , 0.00869976, 0.00849777, 0.0097476 , 0.01153049,\n",
       "        0.00624915, 0.01118265, 0.00904221, 0.00930819, 0.00753717,\n",
       "        0.01041197, 0.02779753, 0.01791868, 0.00374135, 0.00631273,\n",
       "        0.00838358, 0.01140133, 0.00784893, 0.00816603, 0.01371928,\n",
       "        0.00823147, 0.01000986, 0.01099904, 0.02707513, 0.0086493 ,\n",
       "        0.0096023 , 0.01169971, 0.02925974, 0.01073381, 0.01124949,\n",
       "        0.014367  , 0.02534469, 0.01114826, 0.01476666, 0.01537749,\n",
       "        0.02282745, 0.00995813, 0.01444428, 0.02035617, 0.03456584,\n",
       "        0.01223116, 0.01248364, 0.01536351, 0.04451111, 0.01000755,\n",
       "        0.00494077, 0.00556821, 0.01233739, 0.0060789 , 0.00810885,\n",
       "        0.01025804, 0.0114314 , 0.01027597, 0.00528685, 0.00631231,\n",
       "        0.01139528, 0.01126502, 0.00947364, 0.01252055, 0.01928829,\n",
       "        0.00385212, 0.00472848, 0.00737693, 0.01094033, 0.00959363,\n",
       "        0.00648407, 0.00890433, 0.01473832, 0.01430015, 0.00773956,\n",
       "        0.00990816, 0.01565963, 0.00611652, 0.00772587, 0.01111286,\n",
       "        0.01540701]),\n",
       " 'mean_score_time': array([0.00520103, 0.00420134, 0.00420074, 0.00450065, 0.00400023,\n",
       "        0.00400078, 0.00430079, 0.00440226, 0.00400078, 0.00410078,\n",
       "        0.0039011 , 0.00420113, 0.00410047, 0.00380065, 0.00410085,\n",
       "        0.00380077, 0.00400012, 0.00380077, 0.00389998, 0.00390093,\n",
       "        0.00380049, 0.00390038, 0.003901  , 0.00410128, 0.00390105,\n",
       "        0.00390084, 0.00400071, 0.00410066, 0.00400033, 0.0038003 ,\n",
       "        0.00400078, 0.00409999, 0.0041003 , 0.00410039, 0.00410066,\n",
       "        0.00420065, 0.00390098, 0.00370083, 0.00400064, 0.00420096,\n",
       "        0.0040009 , 0.00410128, 0.00390067, 0.00440128, 0.00420094,\n",
       "        0.0040005 , 0.00410068, 0.004001  , 0.00410087, 0.00390115,\n",
       "        0.00410087, 0.0039006 , 0.00380061, 0.00390108, 0.00380092,\n",
       "        0.00420146, 0.00360081, 0.00400023, 0.0039005 , 0.00380089,\n",
       "        0.0040005 , 0.00400064, 0.00400074, 0.00390089, 0.0040009 ,\n",
       "        0.00400107, 0.00440052, 0.00440059, 0.00400031, 0.00410101,\n",
       "        0.00450082, 0.00470088, 0.00430117, 0.00400074, 0.00429788,\n",
       "        0.00400145, 0.00430105, 0.00410066, 0.0050014 , 0.00450108,\n",
       "        0.00410049, 0.00380077, 0.00410082, 0.00410123, 0.00380058,\n",
       "        0.00390081, 0.00410082, 0.00380054, 0.00420017, 0.00400112,\n",
       "        0.00380087, 0.00400071, 0.00400097, 0.00390048, 0.00400064,\n",
       "        0.00380082]),\n",
       " 'std_score_time': array([1.88661150e-03, 3.99983575e-04, 6.00203805e-04, 6.70790284e-04,\n",
       "        6.32561019e-04, 7.78063243e-07, 4.58443936e-04, 4.90195211e-04,\n",
       "        5.25062455e-07, 5.38234454e-04, 3.00281328e-04, 4.00627641e-04,\n",
       "        5.38380755e-04, 4.00020088e-04, 2.99994942e-04, 3.99720935e-04,\n",
       "        4.52995300e-07, 3.99900858e-04, 5.37907221e-04, 3.00543543e-04,\n",
       "        3.99638758e-04, 2.99565638e-04, 3.00090865e-04, 5.39027077e-04,\n",
       "        9.43775377e-04, 2.99716603e-04, 4.46808908e-04, 3.00059101e-04,\n",
       "        4.47395144e-04, 4.00198510e-04, 1.04060902e-06, 5.38337402e-04,\n",
       "        3.00019927e-04, 2.99986572e-04, 3.00615693e-04, 4.00090740e-04,\n",
       "        3.00320988e-04, 4.58162789e-04, 4.47288676e-04, 9.80173543e-04,\n",
       "        6.30796268e-07, 3.00331693e-04, 3.00456191e-04, 4.89824087e-04,\n",
       "        4.00722696e-04, 4.47021765e-04, 6.99946772e-04, 4.46968559e-04,\n",
       "        3.00226612e-04, 5.38610960e-04, 2.99907432e-04, 3.00829323e-04,\n",
       "        4.00353816e-04, 3.00034527e-04, 4.00091393e-04, 4.00222338e-04,\n",
       "        4.90300634e-04, 8.84401178e-07, 7.00300650e-04, 4.00079653e-04,\n",
       "        6.99586904e-07, 1.00277619e-06, 4.47182365e-04, 5.38784372e-04,\n",
       "        4.47075181e-04, 4.47181627e-04, 9.16579255e-04, 9.17068362e-04,\n",
       "        4.46861733e-04, 5.38190159e-04, 1.02485374e-03, 8.99863388e-04,\n",
       "        4.58246887e-04, 4.47074978e-04, 4.53494076e-04, 4.47290069e-04,\n",
       "        4.58531424e-04, 5.38522745e-04, 1.41451593e-03, 8.05679911e-04,\n",
       "        5.37844805e-04, 4.00138580e-04, 2.99050534e-04, 3.00106412e-04,\n",
       "        4.00402675e-04, 2.99708993e-04, 2.99527690e-04, 3.99960042e-04,\n",
       "        7.48277429e-04, 7.10871292e-07, 6.00020654e-04, 6.46813391e-07,\n",
       "        4.47555849e-04, 2.99915918e-04, 4.47395274e-04, 4.00581015e-04]),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6,\n",
       "                    6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 2, 2, 2, 2,\n",
       "                    3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7,\n",
       "                    7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8,\n",
       "                    8, 8, 9, 9, 9, 9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[60, 100, 140, 180, 60, 100, 140, 180, 60, 100, 140,\n",
       "                    180, 60, 100, 140, 180, 60, 100, 140, 180, 60, 100,\n",
       "                    140, 180, 60, 100, 140, 180, 60, 100, 140, 180, 60,\n",
       "                    100, 140, 180, 60, 100, 140, 180, 60, 100, 140, 180,\n",
       "                    60, 100, 140, 180, 60, 100, 140, 180, 60, 100, 140,\n",
       "                    180, 60, 100, 140, 180, 60, 100, 140, 180, 60, 100,\n",
       "                    140, 180, 60, 100, 140, 180, 60, 100, 140, 180, 60,\n",
       "                    100, 140, 180, 60, 100, 140, 180, 60, 100, 140, 180,\n",
       "                    60, 100, 140, 180, 60, 100, 140, 180],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 180},\n",
       "  {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 60},\n",
       "  {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 140},\n",
       "  {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 180}],\n",
       " 'split0_test_score': array([0.98976608, 0.99122807, 0.99122807, 0.98976608, 0.99122807,\n",
       "        0.99122807, 0.99122807, 0.99269006, 0.98830409, 0.99122807,\n",
       "        0.99122807, 0.99122807, 0.98830409, 0.98830409, 0.98830409,\n",
       "        0.98830409, 0.99122807, 0.99122807, 0.98976608, 0.98976608,\n",
       "        0.99122807, 0.99122807, 0.98976608, 0.98976608, 0.99122807,\n",
       "        0.99122807, 0.98976608, 0.98976608, 0.99122807, 0.99122807,\n",
       "        0.98976608, 0.98976608, 0.94517544, 0.97807018, 0.98172515,\n",
       "        0.98245614, 0.94298246, 0.98026316, 0.98391813, 0.98538012,\n",
       "        0.94371345, 0.95248538, 0.98538012, 0.98684211, 0.94517544,\n",
       "        0.95248538, 0.98245614, 0.98684211, 0.94517544, 0.95248538,\n",
       "        0.98245614, 0.98684211, 0.94517544, 0.95248538, 0.98245614,\n",
       "        0.98684211, 0.94517544, 0.95248538, 0.98245614, 0.98684211,\n",
       "        0.94517544, 0.95248538, 0.98245614, 0.98684211, 0.98538012,\n",
       "        0.98830409, 0.98976608, 0.98976608, 0.98684211, 0.99122807,\n",
       "        0.99122807, 0.99122807, 0.98684211, 0.98976608, 0.99122807,\n",
       "        0.99122807, 0.98684211, 0.98976608, 0.99122807, 0.99122807,\n",
       "        0.98684211, 0.98976608, 0.99122807, 0.98976608, 0.98684211,\n",
       "        0.98976608, 0.99122807, 0.98976608, 0.98684211, 0.98976608,\n",
       "        0.99122807, 0.98976608, 0.98684211, 0.98976608, 0.99122807,\n",
       "        0.98976608]),\n",
       " 'split1_test_score': array([0.97222222, 0.96929825, 0.96783626, 0.96345029, 0.97002924,\n",
       "        0.96637427, 0.96491228, 0.96052632, 0.97368421, 0.96637427,\n",
       "        0.9619883 , 0.96052632, 0.97076023, 0.96345029, 0.95906433,\n",
       "        0.95760234, 0.97368421, 0.96637427, 0.96052632, 0.95760234,\n",
       "        0.97222222, 0.96637427, 0.96052632, 0.95467836, 0.97222222,\n",
       "        0.96637427, 0.96052632, 0.95467836, 0.97222222, 0.96637427,\n",
       "        0.96052632, 0.95467836, 0.95760234, 0.95760234, 0.95760234,\n",
       "        0.95760234, 0.95760234, 0.95760234, 0.96125731, 0.9627193 ,\n",
       "        0.95760234, 0.95760234, 0.95467836, 0.95467836, 0.95760234,\n",
       "        0.95760234, 0.95467836, 0.95979532, 0.95760234, 0.95760234,\n",
       "        0.95467836, 0.95979532, 0.95760234, 0.95760234, 0.95467836,\n",
       "        0.95979532, 0.95760234, 0.95760234, 0.95467836, 0.95979532,\n",
       "        0.95760234, 0.95760234, 0.95467836, 0.95979532, 0.96418129,\n",
       "        0.97295322, 0.97076023, 0.96929825, 0.96345029, 0.97002924,\n",
       "        0.96929825, 0.96637427, 0.96783626, 0.97076023, 0.97076023,\n",
       "        0.96783626, 0.95906433, 0.96929825, 0.97222222, 0.96929825,\n",
       "        0.96052632, 0.96929825, 0.97368421, 0.97222222, 0.96052632,\n",
       "        0.96929825, 0.97368421, 0.97222222, 0.96052632, 0.96929825,\n",
       "        0.97368421, 0.97222222, 0.96052632, 0.96929825, 0.97368421,\n",
       "        0.97222222]),\n",
       " 'split2_test_score': array([0.99707602, 0.99707602, 0.99707602, 0.99707602, 0.99415205,\n",
       "        0.99415205, 0.99269006, 0.98976608, 0.99707602, 0.99561404,\n",
       "        0.99561404, 0.99561404, 0.99707602, 0.99122807, 0.99122807,\n",
       "        0.99122807, 0.99561404, 0.99269006, 0.99269006, 0.99269006,\n",
       "        0.99561404, 0.99269006, 0.99269006, 0.99122807, 0.99561404,\n",
       "        0.99269006, 0.99269006, 0.99122807, 0.99561404, 0.99269006,\n",
       "        0.99269006, 0.99122807, 0.97880117, 0.98391813, 0.98538012,\n",
       "        0.98538012, 0.97880117, 0.97880117, 0.98538012, 0.98538012,\n",
       "        0.98464912, 0.98611111, 0.9875731 , 0.9875731 , 0.98318713,\n",
       "        0.98611111, 0.99049708, 0.99122807, 0.98464912, 0.99049708,\n",
       "        0.99195906, 0.99269006, 0.98464912, 0.99049708, 0.99195906,\n",
       "        0.99269006, 0.98464912, 0.99049708, 0.99195906, 0.99269006,\n",
       "        0.98464912, 0.99049708, 0.99195906, 0.99269006, 0.98830409,\n",
       "        0.99561404, 0.99707602, 0.99707602, 0.98391813, 0.99269006,\n",
       "        0.99561404, 0.99415205, 0.99269006, 0.99415205, 0.99707602,\n",
       "        0.99269006, 0.99269006, 0.99707602, 0.99707602, 0.99707602,\n",
       "        0.99269006, 0.99561404, 0.99561404, 0.99561404, 0.99122807,\n",
       "        0.99561404, 0.99561404, 0.99415205, 0.99122807, 0.99561404,\n",
       "        0.99561404, 0.99415205, 0.99122807, 0.99561404, 0.99561404,\n",
       "        0.99415205]),\n",
       " 'split3_test_score': array([1.        , 0.99853801, 0.99853801, 0.99853801, 0.99707602,\n",
       "        0.99853801, 1.        , 0.99853801, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.99853801, 0.99853801,\n",
       "        1.        , 1.        , 1.        , 0.99853801, 1.        ,\n",
       "        1.        , 1.        , 0.99853801, 1.        , 1.        ,\n",
       "        1.        , 0.99853801, 0.99707602, 0.99707602, 1.        ,\n",
       "        1.        , 0.98391813, 0.99269006, 0.99707602, 0.99707602,\n",
       "        0.99561404, 0.99707602, 0.99707602, 0.99707602, 0.99561404,\n",
       "        0.99707602, 0.99707602, 0.99707602, 0.99561404, 0.99707602,\n",
       "        0.99707602, 0.99707602, 0.99561404, 0.99707602, 0.99707602,\n",
       "        0.99707602, 0.99561404, 0.99707602, 0.99707602, 0.99707602,\n",
       "        0.99561404, 0.99707602, 0.99707602, 0.99707602, 1.        ,\n",
       "        1.        , 1.        , 0.99853801, 0.99707602, 1.        ,\n",
       "        0.99853801, 0.99853801, 0.99853801, 1.        , 1.        ,\n",
       "        1.        , 0.99853801, 1.        , 1.        , 1.        ,\n",
       "        0.99853801, 1.        , 1.        , 0.99853801, 0.99853801,\n",
       "        1.        , 1.        , 1.        , 0.99853801, 1.        ,\n",
       "        1.        , 1.        , 0.99853801, 1.        , 1.        ,\n",
       "        1.        ]),\n",
       " 'split4_test_score': array([0.98684211, 0.98830409, 0.98976608, 0.98976608, 0.98830409,\n",
       "        0.98830409, 0.98976608, 0.98976608, 0.98830409, 0.98830409,\n",
       "        0.98830409, 0.98830409, 0.98391813, 0.98830409, 0.98684211,\n",
       "        0.98684211, 0.98538012, 0.98830409, 0.98830409, 0.98830409,\n",
       "        0.98684211, 0.98830409, 0.98830409, 0.98684211, 0.98684211,\n",
       "        0.98830409, 0.98830409, 0.98684211, 0.98684211, 0.98830409,\n",
       "        0.98830409, 0.98684211, 0.98026316, 0.97953216, 0.97880117,\n",
       "        0.98318713, 0.98245614, 0.97953216, 0.98464912, 0.98464912,\n",
       "        0.98172515, 0.97880117, 0.97880117, 0.97660819, 0.98245614,\n",
       "        0.97880117, 0.97880117, 0.97880117, 0.98245614, 0.97880117,\n",
       "        0.97880117, 0.97880117, 0.98245614, 0.97880117, 0.97880117,\n",
       "        0.97880117, 0.98245614, 0.97880117, 0.97880117, 0.97880117,\n",
       "        0.98245614, 0.97880117, 0.97880117, 0.97880117, 0.97880117,\n",
       "        0.98538012, 0.99269006, 0.98976608, 0.98245614, 0.98684211,\n",
       "        0.98830409, 0.98684211, 0.97953216, 0.98391813, 0.98538012,\n",
       "        0.98538012, 0.98099415, 0.98684211, 0.98684211, 0.98684211,\n",
       "        0.98391813, 0.98684211, 0.98684211, 0.98391813, 0.98391813,\n",
       "        0.98684211, 0.98538012, 0.98684211, 0.98391813, 0.98684211,\n",
       "        0.98538012, 0.98684211, 0.98391813, 0.98684211, 0.98538012,\n",
       "        0.98684211]),\n",
       " 'split5_test_score': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.99707602, 1.        , 1.        , 1.        ,\n",
       "        0.99853801, 1.        , 1.        , 1.        , 0.99853801,\n",
       "        1.        , 1.        , 1.        , 0.99707602, 1.        ,\n",
       "        1.        , 1.        , 0.99707602, 1.        , 1.        ,\n",
       "        1.        , 0.99707602, 1.        , 1.        , 1.        ,\n",
       "        0.99707602, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        ]),\n",
       " 'split6_test_score': array([0.98796992, 0.98796992, 0.98646617, 0.98345865, 0.98947368,\n",
       "        0.9924812 , 0.99097744, 0.99097744, 0.98947368, 0.9924812 ,\n",
       "        0.9924812 , 0.9924812 , 0.99398496, 0.99398496, 0.9924812 ,\n",
       "        0.9924812 , 0.98947368, 0.9924812 , 0.9924812 , 0.9924812 ,\n",
       "        0.99022556, 0.99097744, 0.9924812 , 0.99097744, 0.99097744,\n",
       "        0.99097744, 0.9924812 , 0.99097744, 0.99097744, 0.99097744,\n",
       "        0.9924812 , 0.99097744, 0.97894737, 0.98270677, 0.98571429,\n",
       "        0.98270677, 0.95413534, 0.97669173, 0.98345865, 0.98195489,\n",
       "        0.95263158, 0.97819549, 0.98421053, 0.98571429, 0.95263158,\n",
       "        0.98120301, 0.98496241, 0.98496241, 0.95263158, 0.98120301,\n",
       "        0.98496241, 0.98796992, 0.95263158, 0.98120301, 0.98496241,\n",
       "        0.98646617, 0.95263158, 0.98120301, 0.98496241, 0.98646617,\n",
       "        0.95263158, 0.98120301, 0.98496241, 0.98646617, 0.98721805,\n",
       "        0.98796992, 0.98796992, 0.98646617, 0.98421053, 0.98796992,\n",
       "        0.98947368, 0.9924812 , 0.98721805, 0.98796992, 0.98947368,\n",
       "        0.9924812 , 0.9924812 , 0.99398496, 0.9924812 , 0.9924812 ,\n",
       "        0.9924812 , 0.9924812 , 0.99097744, 0.9924812 , 0.99398496,\n",
       "        0.9924812 , 0.9924812 , 0.9924812 , 0.99398496, 0.9924812 ,\n",
       "        0.99097744, 0.9924812 , 0.99398496, 0.9924812 , 0.99097744,\n",
       "        0.9924812 ]),\n",
       " 'split7_test_score': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.99849624, 0.99849624,\n",
       "        1.        , 1.        , 0.99699248, 0.99849624, 1.        ,\n",
       "        1.        , 0.99699248, 0.99849624, 1.        , 1.        ,\n",
       "        0.99699248, 0.99849624, 0.96165414, 0.97142857, 0.97593985,\n",
       "        0.98195489, 0.97593985, 0.97894737, 0.98796992, 0.99097744,\n",
       "        0.98496241, 0.98496241, 0.98796992, 0.99097744, 0.98496241,\n",
       "        0.98646617, 0.98947368, 0.99097744, 0.98496241, 0.98496241,\n",
       "        0.99097744, 0.99097744, 0.98496241, 0.98496241, 0.99097744,\n",
       "        0.99097744, 0.98496241, 0.98496241, 0.99097744, 0.99097744,\n",
       "        0.98496241, 0.98496241, 0.99097744, 0.99097744, 0.98947368,\n",
       "        0.99699248, 1.        , 1.        , 0.99097744, 0.99849624,\n",
       "        1.        , 1.        , 0.9924812 , 0.99849624, 1.        ,\n",
       "        1.        , 0.99398496, 0.99849624, 1.        , 1.        ,\n",
       "        0.99398496, 0.99849624, 1.        , 1.        , 0.9924812 ,\n",
       "        0.99849624, 1.        , 1.        , 0.9924812 , 0.99849624,\n",
       "        1.        , 1.        , 0.9924812 , 0.99849624, 1.        ,\n",
       "        1.        ]),\n",
       " 'split8_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split9_test_score': array([0.99382716, 0.98611111, 0.98611111, 0.98765432, 0.98148148,\n",
       "        0.98302469, 0.98302469, 0.98611111, 0.98148148, 0.98611111,\n",
       "        0.98765432, 0.98765432, 0.98302469, 0.98765432, 0.98919753,\n",
       "        0.98919753, 0.98919753, 0.98919753, 0.98919753, 0.98919753,\n",
       "        0.98611111, 0.98611111, 0.98611111, 0.98611111, 0.98611111,\n",
       "        0.98611111, 0.98611111, 0.98611111, 0.98611111, 0.98611111,\n",
       "        0.98611111, 0.98611111, 0.97376543, 0.97993827, 0.98765432,\n",
       "        0.99228395, 0.95833333, 0.98765432, 0.98611111, 0.98611111,\n",
       "        0.96064815, 0.98611111, 0.98765432, 0.98919753, 0.95987654,\n",
       "        0.9845679 , 0.9845679 , 0.98611111, 0.95987654, 0.9845679 ,\n",
       "        0.98765432, 0.98765432, 0.95987654, 0.9845679 , 0.98765432,\n",
       "        0.98611111, 0.95987654, 0.9845679 , 0.98765432, 0.98611111,\n",
       "        0.95987654, 0.9845679 , 0.98765432, 0.98611111, 0.99228395,\n",
       "        0.99382716, 0.99382716, 0.99074074, 0.98765432, 0.98611111,\n",
       "        0.98302469, 0.9845679 , 0.98611111, 0.98148148, 0.98302469,\n",
       "        0.98611111, 0.9845679 , 0.9845679 , 0.98302469, 0.98919753,\n",
       "        0.98765432, 0.98148148, 0.98148148, 0.98302469, 0.9845679 ,\n",
       "        0.98302469, 0.98302469, 0.98611111, 0.9845679 , 0.98302469,\n",
       "        0.98302469, 0.98611111, 0.9845679 , 0.98302469, 0.98302469,\n",
       "        0.98611111]),\n",
       " 'mean_test_score': array([0.99277035, 0.99185255, 0.99170217, 0.99097095, 0.99117446,\n",
       "        0.99141024, 0.99125986, 0.99083751, 0.99183236, 0.99201128,\n",
       "        0.991727  , 0.9915808 , 0.99170681, 0.99129258, 0.99071173,\n",
       "        0.99056553, 0.99245776, 0.99202752, 0.99099995, 0.99070756,\n",
       "        0.99222431, 0.9915685 , 0.99068713, 0.98966374, 0.9922995 ,\n",
       "        0.9915685 , 0.99068713, 0.98966374, 0.9922995 , 0.9915685 ,\n",
       "        0.99068713, 0.98966374, 0.97732851, 0.98302724, 0.98528172,\n",
       "        0.98655713, 0.97312448, 0.98321823, 0.98698204, 0.98742481,\n",
       "        0.97600842, 0.9821345 , 0.98633435, 0.9868667 , 0.97600436,\n",
       "        0.98243131, 0.98625128, 0.98757937, 0.97600436, 0.98271953,\n",
       "        0.98685649, 0.98818064, 0.97600436, 0.98271953, 0.98685649,\n",
       "        0.98787594, 0.97600436, 0.98271953, 0.98685649, 0.98787594,\n",
       "        0.97600436, 0.98271953, 0.98685649, 0.98787594, 0.98856423,\n",
       "        0.9921041 , 0.99320895, 0.99216514, 0.9876585 , 0.99133668,\n",
       "        0.99154808, 0.99141836, 0.9891249 , 0.99065441, 0.99169428,\n",
       "        0.99157268, 0.98891627, 0.99200316, 0.99228743, 0.99261232,\n",
       "        0.98966351, 0.99139794, 0.99198273, 0.99155644, 0.98920867,\n",
       "        0.99155226, 0.99214123, 0.99215748, 0.98920867, 0.99155226,\n",
       "        0.99199086, 0.99215748, 0.98920867, 0.99155226, 0.99199086,\n",
       "        0.99215748]),\n",
       " 'std_test_score': array([0.00847447, 0.00916973, 0.00956242, 0.01080198, 0.00910809,\n",
       "        0.00993791, 0.01035341, 0.01121609, 0.00872522, 0.00987372,\n",
       "        0.01096535, 0.0113634 , 0.00946312, 0.01056798, 0.01171797,\n",
       "        0.01211432, 0.00813074, 0.00967206, 0.01104876, 0.01186024,\n",
       "        0.0085132 , 0.00980445, 0.0111549 , 0.01272472, 0.00849852,\n",
       "        0.00980445, 0.0111549 , 0.01272472, 0.00849852, 0.00980445,\n",
       "        0.0111549 , 0.01272472, 0.01764279, 0.01265582, 0.0125051 ,\n",
       "        0.0121609 , 0.01807913, 0.01196449, 0.01064955, 0.01039284,\n",
       "        0.01954287, 0.01552015, 0.01248642, 0.01270701, 0.01933135,\n",
       "        0.01547286, 0.0126289 , 0.0113232 , 0.01922602, 0.01560418,\n",
       "        0.01271561, 0.01132472, 0.01922602, 0.01560418, 0.01271561,\n",
       "        0.01135106, 0.01922602, 0.01560418, 0.01271561, 0.01135106,\n",
       "        0.01922602, 0.01560418, 0.01271561, 0.01135106, 0.01053023,\n",
       "        0.00818364, 0.00862163, 0.00904322, 0.01021752, 0.00889445,\n",
       "        0.00928616, 0.00987593, 0.00955131, 0.00932224, 0.00929148,\n",
       "        0.00958053, 0.01169627, 0.00930649, 0.00883226, 0.00910314,\n",
       "        0.01105531, 0.00949459, 0.00862213, 0.00887747, 0.01111462,\n",
       "        0.0093435 , 0.00854129, 0.00847378, 0.01111462, 0.0093435 ,\n",
       "        0.00854722, 0.00847378, 0.01111462, 0.0093435 , 0.00854722,\n",
       "        0.00847378]),\n",
       " 'rank_test_score': array([ 2, 21, 25, 45, 43, 38, 42, 46, 22, 16, 23, 27, 24, 41, 47, 53,  4,\n",
       "        15, 44, 48,  8, 31, 49, 54,  5, 29, 49, 54,  5, 29, 49, 54, 89, 82,\n",
       "        80, 77, 96, 81, 71, 70, 90, 88, 78, 72, 91, 87, 79, 69, 91, 83, 73,\n",
       "        64, 91, 83, 73, 65, 91, 83, 73, 65, 91, 83, 73, 65, 63, 14,  1,  9,\n",
       "        68, 40, 36, 37, 61, 52, 26, 28, 62, 17,  7,  3, 57, 39, 20, 32, 58,\n",
       "        33, 13, 10, 58, 33, 18, 10, 58, 33, 18, 10])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "xg_boost.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
